# -*- coding: utf-8 -*-
"""Project_AAI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GmojfwUq-g-Qszf4QpMqYvK7bYbkPjg_

Project of Advanced Artificial Intelligence course - Extracting independence from medical datasey and building Bayesian model

Student name: Madina Kudaibergenova

Project: Building BN for medical dataset
"""

# please, before running the code below, install and restart the notebook
# because these frameworks needs to be installed to show plots right
!pip install arviz==0.6.1
!pip install pymc3==3.8
!pip install Theano==1.0.4

"""# 1. Removing unlabeled data"""

# due to that I used Google colab and to read data (and keep it) from drive 
# I called these functions below; however, please, change them if necessary
from google.colab import drive
drive.mount('/content/gdrive')

# importing necessary library
import pandas as pd
import seaborn as sn
import matplotlib.pyplot as plt
import numpy as np

# also change the data paths to work it properly
d1 = r"/content/gdrive/MyDrive/Masters CS/Advanced AI/Project/features_all_images.csv"
d2 = r"/content/gdrive/MyDrive/Masters CS/Advanced AI/Project/Labels for data (higher is better).csv"

# reading csv files
df1 = pd.read_csv(d1) 
df2 = pd.read_csv(d2)

# variables to store list of patientID to search for labeled ones
patient1 = set(df1['patient'].values.tolist()) 
patient2 = set(df2['patientId'].values.tolist())

# find intersection in both datasets (labeled and unlabeled)
sim_patients = patient1.intersection(patient2)

# sort patientIDs for convenience
df1.sort_values(by='patient')
df2.sort_values(by='patientId')
iseq = df2['patientId'] == df1[:len(df2)]['patient']
csv = pd.DataFrame(df2[iseq])

# resulting dataset of labeled patients with 86 features
df_similar = df1[df1.patient.isin(df2.patientId)]
df_similar

"""# 2. Choosing uncorrelated data"""

# find a pairwise correlation in the dataset
df_similar_corr = df_similar.corr()
df_similar_corr

# drawing correlation matrix
colormap = plt.cm.cubehelix_r
sn.heatmap(df_similar_corr, annot=False, cmap=colormap)
plt.figure(figsize=(15,15))
plt.show()

# import library to filter dataset
from sklearn.feature_selection import VarianceThreshold

# removing constant features with 0 variance
constant_filter = VarianceThreshold(threshold=0)
constant_filter.fit(df_similar_corr)

# see if there is any constant variables in our dataset
constant_columns = [column for column in df_similar_corr.columns
                    if column not in df_similar_corr.columns[constant_filter.get_support()]]

print(len(constant_columns))

# find duplicates 
print(df_similar_corr.T.duplicated().sum())

# drop duplicates
unique_features = df_similar_corr.T.drop_duplicates(keep='first').T

duplicated_features = [dup_col for dup_col in df_similar_corr.columns if dup_col not in unique_features.columns]
duplicated_features

# removing correlated features
correlated_features = set()
for i in range(len(df_similar_corr.columns)):
    for j in range(i):
        if abs(df_similar_corr.iloc[i, j]) > 0.8:
            colname = df_similar_corr.columns[i]
            correlated_features.add(colname)

len(correlated_features)

print(correlated_features)

# drop correlated features
df_similar.drop(labels=correlated_features, axis=1, inplace=True)
df_similar

"""# 3. Merging labels with features"""

# choose target algorithm with highest score
df2['maxAccuracy'] = df2[['chexnet','resnet','unet','cnn']].max(axis=1)
df2['algorithm'] = df2[['chexnet','resnet','unet','cnn']].idxmax(axis=1)

# delete unnecessary columns
df2.pop('chexnet')
df2.pop('resnet')
df2.pop('unet')
df2.pop('cnn')

# our dataset with labeled data
df2

# create new dataframe
new = df2[['maxAccuracy','algorithm']].copy()
new

# copying columns
df_similar[['maxA','algo']] = df2[['maxAccuracy','algorithm']].values

df_similar

# save results to new csv file - not necessary
df_similar.to_csv('results.csv')

"""# 4. Naive Bayes Classification"""

# import libraries
import time
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB

# Convert categorical variable to numeric
df_similar['algo_cleaned']=np.where(df_similar['maxA']==1,1,0)

# Split dataset in training and test datasets
X_train, X_test = train_test_split(df_similar, test_size=0.5, random_state=int(time.time()))

# Instantiate the classifier
gnb = GaussianNB()
used_features =[
    'original_firstorder_10Percentile',
    'original_firstorder_90Percentile',
    'original_firstorder_Energy',
    'original_firstorder_Entropy',
    'original_firstorder_Kurtosis',
    'original_firstorder_Maximum',
    'original_firstorder_Skewness',
    'original_glcm_Autocorrelation',
    'original_glcm_Contrast',
    'original_glcm_Correlation',
    'original_glcm_Imc1',
    'original_glcm_Idmn',
    'original_glrlm_GrayLevelNonUniformity',
    'original_glrlm_LongRunHighGrayLevelEmphasis',
    'original_glrlm_LongRunLowGrayLevelEmphasis',
    'original_glszm_SizeZoneNonUniformity',
    'original_glszm_SizeZoneNonUniformityNormalized',
    'original_gldm_DependenceNonUniformity',
    'original_gldm_GrayLevelNonUniformity',
    'algo_cleaned'
]

# Train classifier
gnb.fit(
    X_train[used_features].values,
    X_train["algo_cleaned"]
)
y_pred = gnb.predict(X_test[used_features])

# Print results
print("Number of mislabeled points out of a total {} points : {}, performance {:05.2f}%"
      .format(
          X_test.shape[0],
          (X_test["algo_cleaned"] != y_pred).sum(),
          100*(1-(X_test["algo_cleaned"] != y_pred).sum()/X_test.shape[0])
))

# apply  classifier on train data
mean_algo=np.mean(X_train["algo_cleaned"])
mean_not_algo=1-mean_algo
print("Correct algo prob = {:03.2f}%, Not correct algo prob = {:03.2f}%"
      .format(100*mean_algo,100*mean_not_algo))

# find probability distributions P(original_firstorder_10Percentile | algo = 0) and P(original_firstorder_10Percentile | algo = 1)
# mean and std values
mean_patient_algo = np.mean(X_train[X_train["algo_cleaned"]==1]["original_firstorder_10Percentile"])
std_patient_algo = np.std(X_train[X_train["algo_cleaned"]==1]["original_firstorder_10Percentile"])
mean_patient_not_algo = np.mean(X_train[X_train["algo_cleaned"]==0]["original_firstorder_10Percentile"])
std_patient_not_algo = np.std(X_train[X_train["algo_cleaned"]==0]["original_firstorder_10Percentile"])

print("mean_patient_algo = {:03.2f}".format(mean_patient_algo))
print("std_patient_algo = {:03.2f}".format(std_patient_algo))
print("mean_patient_not_algo = {:03.2f}".format(mean_patient_not_algo))
print("std_patient_not_algo = {:03.2f}".format(std_patient_not_algo))

# histogram
gnb = GaussianNB()
used_features =["original_firstorder_10Percentile"]
y_pred = gnb.fit(X_train[used_features].values, X_train["algo_cleaned"]).predict(X_test[used_features])
print("Number of mislabeled points out of a total {} points : {}, performance {:05.2f}%"
      .format(
          X_test.shape[0],
          (X_test["algo_cleaned"] != y_pred).sum(),
          100*(1-(X_test["algo_cleaned"] != y_pred).sum()/X_test.shape[0])
))
print("Std original_firstorder_10Percentile not_algo: {:05.2f}".format(np.sqrt(gnb.sigma_)[0][0]))
print("Std original_firstorder_10Percentile algo: {:05.2f}".format(np.sqrt(gnb.sigma_)[1][0]))
print("Mean original_firstorder_10Percentile not_algo {:05.2f}".format(gnb.theta_[0][0]))
print("Mean original_firstorder_10Percentile algo: {:05.2f}".format(gnb.theta_[1][0]))

"""# 5. Analysing and Building BN"""

# import libraries
from scipy import stats
import arviz as az
import pymc3 as pm
from theano import shared
from sklearn import preprocessing

# check if there is any missing data
df_similar.isnull().sum()/len(df_similar)

# Gaussian inference
# KDE plot pf the kurtosis shows a Gaussian-like distribution
az.plot_kde(df_similar['maxA'].values, rug=True)
plt.yticks([0], alpha=0);

# actual mean
print(np.mean(df_similar['maxA']))

# build Model and sample
with pm.Model() as model_g:
    μ = pm.Uniform('μ', lower=0, upper=200)
    σ = pm.HalfNormal('σ', sd=58.8)
    # y - is log-likelyhood
    y = pm.Normal('y', mu=μ, sd=σ, 
      observed=df_similar['original_firstorder_10Percentile'].values)
    trace_g = pm.sample(1000, tune=1000)

# KDE plot and sampled plot
az.plot_trace(trace_g);

# plotting joint distributions of parameters
az.plot_joint(trace_g, kind='kde', fill_last=False)

# Posterior Predictive Checks
# randomly draw 1000 samples of parameters from the trace, and for each sample it will draw 231 random numbers
# from a normal distribution specified by the values of mu and std in that sample
ppc = pm.sample_posterior_predictive(trace_g, samples=1000, model = model_g)
np.asarray(ppc['y']).shape

_, ax = plt.subplots(figsize=(10,5))
ax.hist([y.mean() for y in ppc['y']], bins=19, alpha=0.5)
ax.axvline(res_dfs_and_df2.maxAccuracy.mean())
ax.set(title='Posterior predictive of the mean', xlabel='mean(x)', ylabel = 'Frequency')